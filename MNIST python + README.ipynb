{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f289007",
   "metadata": {},
   "source": [
    "#  Written digit recogniser (MNIST) - neural network\n",
    "\n",
    "This neural network was reimplimented from this youtube video https://www.youtube.com/watch?v=w8yWXqWQYmU\n",
    "\n",
    "The MNIST database can be found here https://www.kaggle.com/competitions/digit-recognizer",
    "\n",
    "It is a simple neural network with 3 layers: an input layer, 1 hidden layer and an output layer. \n",
    "\n",
    "It uses the MNIST database to make predictions on handwritten numbers from 0-9 (10 labels). \n",
    "\n",
    "Each image is a matrix of 784 pixel values (28x28 image)\n",
    "\n",
    "There are 42000 images/examples\n",
    "\n",
    "As I am starting a neural networks class I have written descriptions for every step in this project to aid my learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5e12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bf033",
   "metadata": {},
   "source": [
    "## Load data\n",
    "The pixel matrix is divided by 255 as each pixel has a value between 0 & 255, 0 is black & 255 is white so dividing each pixel value by 255 scales the data by giving each a value between 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec484e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data = np.array(data) # creates a 785 x n_examples matrix (784 columns for pixels + 1 column for labels 0-9)\n",
    "n_examples, pixel = data.shape # creates a variable for the number of examples and pixels\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data_train = data[1000:n_examples].T # transpose matrix so pixels are rows\n",
    "Y_train = data_train[0] # label vector\n",
    "X_train = data_train[1:pixel] # pixels matrix\n",
    "X_train = X_train / 255 \n",
    "X_pixel,n_examplesX = X_train.shape\n",
    "\n",
    "data_test = data[0:1000].T\n",
    "Y_test = data_test[0]\n",
    "X_test = data_test[1:pixel]\n",
    "X_test = X_test / 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ece6b9",
   "metadata": {},
   "source": [
    "## Test that data is loaded and labelled correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b7a9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 41000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # matrix of 784 rows pixels x 41000 columns examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de7483c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape # vector of 42000 labels which correspond to 41000 examples of 784 pixel combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afeba6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add75394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a115c",
   "metadata": {},
   "source": [
    "# Variables/matrices and their dimensions\n",
    "(rows x columns)\n",
    "\n",
    "### Forward propagation\n",
    "$ A^{0} $ = X = 784 x n_examples = input\n",
    "\n",
    "$ Z^{1} $ = 10 x n_examples = first hidden layer\n",
    "\n",
    "$ A^{1} $ = 10 x n_examples = ReLU activation function applied in first hidden layer\n",
    "\n",
    "$ W^{1} $ = 10 x 784 = weight in first hidden layer\n",
    "\n",
    "$ B^{1} $ = 10 x 1 = bias in first hidden layer \n",
    "\n",
    "$ Z^{2} $ = 10 x n_examples = output\n",
    "\n",
    "$ A^{2} $ = 10 x n_examples = softmax function applied to output\n",
    "\n",
    "$ W^{2} $ = 10 x 10 = weight applied to output\n",
    "\n",
    "$ B^{2} $ = 10 x 1 = bias applied to output\n",
    "\n",
    "\n",
    "### Backwards propagation\n",
    "$ dZ^{2} $ = 10 x n_examples = applied to output\n",
    "\n",
    "$ dW^{2} $ = 10 x 10\n",
    "\n",
    "$ db^{2} $ = 10 x 1\n",
    "\n",
    "$ dZ^{1} $ = 10 x n_examples\n",
    "\n",
    "$ dW^{1} $ = 10 x 784\n",
    "\n",
    "$ db^{1} $ = 10 x 1\n",
    "\n",
    "alpha = learning rate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fe144",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "1. $ A^{0} $ is computed via a random weight and bias, $ W^{1} $ & $ b^{1} $ which are arrays with random numbers between -0.5 and 0.5  which then becomes $ Z^{1} $\n",
    "\n",
    "    $ Z^{1} = W^{1}\\cdot A^{0} + b^{1} $\n",
    "    \n",
    "\n",
    "2. $ Z^{1} $ is passed through an activation function, ReLU in this case, and becomes $ A^{1} $\n",
    "\n",
    "    $ A^{1} $ = ReLU($ Z^{1} $) \n",
    "    \n",
    "    ### ReLU Activation Function\n",
    "    \n",
    "    <img src=\"1_DfMRHwxY1gyyDmrIAd-gjQ.png\" width=\"500\" height=\"500\">\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "3. $ A^{1} $ is then computed using a different weight and bias \n",
    "    \n",
    "    $ Z^{2} $ = $ W^{2}\\cdot A^{1} + b^{2} $\n",
    "    \n",
    "    \n",
    "4. $ Z^{2} $ is passed through a different activation function, softmax\n",
    "\n",
    "    $ A^{2} $ = softmax($ Z^{2} $)\n",
    "    \n",
    "    ### The softmax function returns a probability \n",
    "    <img src=\"0ab139bc-3ff6-49d2-8b36-dcc98ef31102.png\" width=\"300\" height=\"300\">\n",
    "    \n",
    "    \n",
    "Softmax function takes the exponentials of each number in the matrix $ Z^{2} $ and sums the exponentials of them\n",
    "\n",
    "The exponential of each number is then divided by the previous sum to form a probability - all numbers will be between 0 and 1\n",
    "\n",
    "the highest probability will be the closest prediction to which label matches the matrix of pixels\n",
    "\n",
    "\n",
    "at this point the numbers generated are taken from random guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb33852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5 # 1st weight parameter, returns matrix of shape 10x784 (10 digits, 784 pixels) with random numbers between -0.5 and 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5 # 1st bias parameter\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z): # ReLU activation function returns Z if Z > 0 and 0 if Z < 0\n",
    "    return np.maximum(Z, 0) \n",
    "\n",
    "def softmax(Z): # softmax activation function returns probability\n",
    "    return np.exp(Z) / sum(np.exp(Z))\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947553c",
   "metadata": {},
   "source": [
    "## Backwards Propagation\n",
    "\n",
    "**Back propogatoin starts from the second/output layer. It takes the random numbers and tries to form weights and biases that have a higher probability of forming the correct answer**\n",
    "\n",
    "\n",
    "1. The first error variable of backprop takes the prediction (output) and subtracts the value of the correct label, so it computes how much the prediction deviated from label\n",
    "\n",
    "    $ dZ^{2} $ = $ A^{2} - Y $\n",
    "\n",
    "    Y is one hot encoded in order to retrieve the value of the index rather than the index number - the value of the index will be changed to either 1 or 0\n",
    "\n",
    "\n",
    "2. The derivative of the weight applied to the output value (before softmax) with respect to the error is computed, so this computes how much the weight caused a change in the error - this will allow the neural network to change the weight in a systematic way\n",
    "\n",
    "    $dW^{2} = \\frac{1}{nexamples} dZ^{2} A^{1T} $\n",
    "\n",
    "\n",
    "3. neural network computes the average for how much it deviated from the correct prediction\n",
    "\n",
    "    $ db^{2} = \\frac{1}{nexamples} \\sum dZ^{2} $\n",
    "\n",
    "\n",
    "**It then works on the first/hidden layer**\n",
    "\n",
    "4. Computes how much the hidden layer contributed to the error in prediction. It multiples the derivative of the first activation function\n",
    "\n",
    "    $ dZ^{1} = (W^{2T} \\cdot dZ^{2})  g'(Z^{1}) $\n",
    "\n",
    "\n",
    "5. computes the average error of $W^{1}$\n",
    "\n",
    "    $ \\frac{1}{nexamples} \\sum dZ^{1} \\cdot X^{T} $\n",
    "    \n",
    "    \n",
    "6. computes the average error of $b^{1}$\n",
    "\n",
    "    $ \\frac{1}{nexamples} \\sum dZ^{1} $\n",
    "\n",
    "\n",
    "once these variables are found the parameters are updated\n",
    "\n",
    "$ W^{1} = W^{1} - \\alpha $ x $ dW^{1} $\n",
    "\n",
    "$ b^{1} = b^{1} - \\alpha $ x $ db^{1} $\n",
    "\n",
    "$ W^{2} = W^{2} - \\alpha $ x $ dW^{2} $\n",
    "\n",
    "$ b^{2} = b^{2} - \\alpha $ x $ db^{2} $\n",
    "\n",
    "\n",
    "$ \\alpha $ is the learning rate set by programmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f55289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1)) # creates a matrix of zeros, dimensions = amount of examples x amount of labels\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1 # goes through every row of matrix one_hot_Y, if the column is y it will be set to 1\n",
    "    one_hot_Y = one_hot_Y.T # transpose so each column is an example and rows are labels\n",
    "    return one_hot_Y\n",
    "\n",
    "def deriv_ReLU(Z): # derivative of ReLU function, for all values below or equal to 0 the slope of the line is 0, for all values 1 and above the slope is 1\n",
    "    return Z > 0\n",
    "\n",
    "def back_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y  \n",
    "    dW2 = 1 / n_examples * dZ2.dot(A1.T)\n",
    "    db2 = 1 / n_examples * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * deriv_ReLU(Z1)\n",
    "    dW1 = 1 / n_examples * dZ1.dot(X.T)\n",
    "    db1 = 1 / n_examples * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad92b6",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "neural network starts again with updated parameters\n",
    "\n",
    "$ A^{2} $ is the output of forward propagation after softmax function applied to $ Z^{2} $ so this is the prediction for which label fits the example matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "126d6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2): # since A2 is a 10xn_examples matrix of probabilities this function returns the value with the highest probability \n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size # average correct predictions\n",
    "\n",
    "def gradient_descent(X, Y, iterations, alpha): # alpha = learning rate\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0: \n",
    "            print(\"Iteration: \", i) \n",
    "            print(\"Accuracy: \", get_accuracy(get_predictions(A2), Y)) # prints accuracy of the prediction every 10th iteration\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74936da7",
   "metadata": {},
   "source": [
    "# Train neural network\n",
    "\n",
    "500 iterations with a 0.1 learning rate\n",
    "\n",
    "It's accuracy is ~ 0.85 or 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a5d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 500, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be762a",
   "metadata": {},
   "source": [
    "With 1000 iterations the NN reaches ~ 0.87 or 87% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ede60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[3 6 0 ... 8 8 8] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.10143902439024391\n",
      "Iteration:  10\n",
      "[3 5 5 ... 6 8 8] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.13807317073170733\n",
      "Iteration:  20\n",
      "[3 5 5 ... 8 8 1] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.20848780487804877\n",
      "Iteration:  30\n",
      "[3 3 6 ... 8 8 1] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.27007317073170733\n",
      "Iteration:  40\n",
      "[3 3 6 ... 8 8 1] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.3307317073170732\n",
      "Iteration:  50\n",
      "[3 3 5 ... 8 8 1] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.3801219512195122\n",
      "Iteration:  60\n",
      "[3 3 5 ... 8 8 1] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.4205121951219512\n",
      "Iteration:  70\n",
      "[3 3 2 ... 1 8 1] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.45365853658536587\n",
      "Iteration:  80\n",
      "[3 3 2 ... 8 8 1] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.4833170731707317\n",
      "Iteration:  90\n",
      "[3 3 8 ... 8 8 1] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.5091463414634146\n",
      "Iteration:  100\n",
      "[3 3 8 ... 8 8 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.5339512195121952\n",
      "Iteration:  110\n",
      "[3 3 8 ... 8 8 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.5567560975609757\n",
      "Iteration:  120\n",
      "[3 3 8 ... 9 8 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.5811463414634146\n",
      "Iteration:  130\n",
      "[3 3 8 ... 9 8 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.6051951219512195\n",
      "Iteration:  140\n",
      "[3 3 8 ... 9 8 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.6268048780487805\n",
      "Iteration:  150\n",
      "[3 3 8 ... 9 8 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.6469756097560976\n",
      "Iteration:  160\n",
      "[3 3 8 ... 9 8 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.6641463414634147\n",
      "Iteration:  170\n",
      "[3 3 8 ... 9 8 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.6801951219512196\n",
      "Iteration:  180\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.6948048780487804\n",
      "Iteration:  190\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7070731707317073\n",
      "Iteration:  200\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7176829268292683\n",
      "Iteration:  210\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7265121951219512\n",
      "Iteration:  220\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7344390243902439\n",
      "Iteration:  230\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7426829268292683\n",
      "Iteration:  240\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.749390243902439\n",
      "Iteration:  250\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7561707317073171\n",
      "Iteration:  260\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7620243902439024\n",
      "Iteration:  270\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.767219512195122\n",
      "Iteration:  280\n",
      "[3 7 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7722439024390244\n",
      "Iteration:  290\n",
      "[3 7 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7767560975609756\n",
      "Iteration:  300\n",
      "[3 7 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7811219512195122\n",
      "Iteration:  310\n",
      "[3 7 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7854878048780488\n",
      "Iteration:  320\n",
      "[3 7 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7894878048780488\n",
      "Iteration:  330\n",
      "[3 7 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.793\n",
      "Iteration:  340\n",
      "[3 7 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7960731707317074\n",
      "Iteration:  350\n",
      "[3 7 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.7994634146341464\n",
      "Iteration:  360\n",
      "[3 7 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8024146341463415\n",
      "Iteration:  370\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8055853658536586\n",
      "Iteration:  380\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8086341463414635\n",
      "Iteration:  390\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8115121951219513\n",
      "Iteration:  400\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8141463414634146\n",
      "Iteration:  410\n",
      "[3 3 8 ... 9 4 9] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8169024390243902\n",
      "Iteration:  420\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8192926829268292\n",
      "Iteration:  430\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8213170731707317\n",
      "Iteration:  440\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8233170731707317\n",
      "Iteration:  450\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.825170731707317\n",
      "Iteration:  460\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8272439024390243\n",
      "Iteration:  470\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8293170731707317\n",
      "Iteration:  480\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8309024390243902\n",
      "Iteration:  490\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.832780487804878\n",
      "Iteration:  500\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8343658536585365\n",
      "Iteration:  510\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8363170731707317\n",
      "Iteration:  520\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8378780487804878\n",
      "Iteration:  530\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8394878048780487\n",
      "Iteration:  540\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.841170731707317\n",
      "Iteration:  550\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8424634146341463\n",
      "Iteration:  560\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8438292682926829\n",
      "Iteration:  570\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.845\n",
      "Iteration:  580\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8463658536585366\n",
      "Iteration:  590\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8474878048780488\n",
      "Iteration:  600\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8483170731707317\n",
      "Iteration:  610\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8492195121951219\n",
      "Iteration:  620\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8502682926829268\n",
      "Iteration:  630\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8515365853658536\n",
      "Iteration:  640\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8522926829268292\n",
      "Iteration:  650\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8535121951219512\n",
      "Iteration:  660\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8544390243902439\n",
      "Iteration:  670\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8551463414634146\n",
      "Iteration:  680\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8564634146341463\n",
      "Iteration:  690\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8571707317073171\n",
      "Iteration:  700\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8579756097560975\n",
      "Iteration:  710\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8586585365853658\n",
      "Iteration:  720\n",
      "[3 3 8 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8595609756097561\n",
      "Iteration:  730\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8605121951219512\n",
      "Iteration:  740\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8614146341463415\n",
      "Iteration:  750\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8624146341463415\n",
      "Iteration:  760\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8630731707317073\n",
      "Iteration:  770\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8637073170731707\n",
      "Iteration:  780\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8642926829268293\n",
      "Iteration:  790\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8649756097560976\n",
      "Iteration:  800\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8657560975609756\n",
      "Iteration:  810\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8663658536585366\n",
      "Iteration:  820\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8667073170731707\n",
      "Iteration:  830\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8672926829268293\n",
      "Iteration:  840\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8681219512195122\n",
      "Iteration:  850\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.868609756097561\n",
      "Iteration:  860\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8692439024390244\n",
      "Iteration:  870\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8697317073170732\n",
      "Iteration:  880\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8700975609756098\n",
      "Iteration:  890\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8707073170731707\n",
      "Iteration:  900\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8713170731707317\n",
      "Iteration:  910\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8719512195121951\n",
      "Iteration:  920\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8722682926829268\n",
      "Iteration:  930\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8727073170731707\n",
      "Iteration:  940\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8730731707317073\n",
      "Iteration:  950\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8738292682926829\n",
      "Iteration:  960\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8743170731707317\n",
      "Iteration:  970\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8746829268292683\n",
      "Iteration:  980\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.8754146341463415\n",
      "Iteration:  990\n",
      "[3 3 0 ... 9 4 2] [3 7 5 ... 9 4 2]\n",
      "Accuracy:  0.875780487804878\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 1000, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca06b5c8",
   "metadata": {},
   "source": [
    "# Run neural network and print out predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94f874ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    X_pixel, X_pixel, X_pixel, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "def test_prediction(index, W1, b1, W2, b2):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n",
    "    label = Y_train[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "    \n",
    "    current_image = current_image.reshape((28,28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "065ef894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [3]\n",
      "Label:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction(236, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21291e7a",
   "metadata": {},
   "source": [
    "# Run neural network on test data\n",
    "\n",
    "accuracy ~ 89% on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d2065b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 9 0 3 7 0 1 1 3 5 6 6 6 2 0 9 6 7 5 1 9 6 1 0 0 6 2 4 6 0 3 1 7 4 5 6\n",
      " 6 1 1 5 5 0 5 2 6 5 2 0 1 1 1 8 3 7 0 4 1 6 4 5 7 3 3 1 7 3 2 4 3 3 4 1 7\n",
      " 8 1 7 7 8 8 9 8 8 5 3 8 0 9 3 3 8 0 1 1 8 4 0 6 1 0 1 2 8 0 7 9 1 8 9 7 8\n",
      " 3 1 9 4 9 2 9 2 8 6 2 7 4 4 7 9 3 8 6 7 5 0 8 4 5 9 0 9 0 3 8 9 6 5 8 5 7\n",
      " 5 1 7 7 7 8 2 5 1 0 7 0 6 7 1 8 7 5 6 0 1 8 1 1 3 6 6 7 6 3 1 6 0 6 9 2 9\n",
      " 9 5 1 3 4 7 6 8 5 9 1 3 2 6 4 9 7 2 5 7 9 1 7 9 0 6 8 7 8 7 1 6 6 0 6 9 1\n",
      " 4 9 8 2 0 9 4 7 7 3 2 4 9 5 0 9 1 5 0 3 7 2 3 7 6 4 1 8 2 1 6 2 5 3 9 6 1\n",
      " 6 9 8 8 9 2 0 8 7 3 9 9 0 1 8 4 1 4 0 6 4 0 2 9 4 7 4 6 7 5 0 6 6 0 2 1 2\n",
      " 6 9 2 8 7 0 5 8 5 3 1 1 0 8 9 0 0 6 6 4 2 4 4 4 2 6 6 2 8 5 2 6 1 9 9 0 7\n",
      " 5 7 5 2 1 8 0 3 4 4 3 8 3 9 7 2 8 8 5 8 1 9 2 9 9 0 0 4 6 3 5 4 8 2 6 2 7\n",
      " 5 2 4 0 8 9 2 6 2 1 1 0 9 9 6 3 9 7 9 3 7 2 1 0 4 1 3 5 9 9 1 4 4 7 4 9 0\n",
      " 5 1 0 4 1 6 7 2 1 9 9 1 4 4 4 9 8 1 4 4 3 7 5 0 6 5 3 8 9 1 0 8 7 3 8 0 5\n",
      " 4 8 7 5 4 5 0 1 1 2 1 5 9 9 2 2 8 7 1 4 0 6 0 5 5 7 3 8 6 8 9 0 1 6 0 9 1\n",
      " 8 7 5 2 2 1 4 7 0 9 2 7 1 4 1 2 8 5 8 1 7 0 6 1 8 1 9 2 4 5 5 9 3 6 9 3 6\n",
      " 6 4 2 7 0 6 4 5 1 9 5 9 5 0 1 9 9 2 9 1 7 0 9 7 9 6 7 7 2 4 4 3 5 5 8 0 7\n",
      " 4 6 1 2 5 7 1 7 2 0 6 7 1 8 0 5 4 0 1 9 7 0 7 2 5 6 7 9 0 5 3 9 8 1 0 3 6\n",
      " 7 4 6 6 7 4 6 7 7 2 3 6 6 7 1 9 2 3 1 4 4 4 9 7 3 4 3 4 0 6 5 0 3 0 1 0 9\n",
      " 9 3 1 5 2 5 3 8 6 2 1 0 7 5 8 8 3 7 5 7 1 2 5 4 1 9 0 3 8 8 1 8 9 9 7 6 8\n",
      " 6 8 2 7 7 6 5 3 9 7 7 2 6 9 0 1 0 2 3 8 3 8 4 0 4 2 6 3 0 7 1 4 6 4 4 1 8\n",
      " 2 9 0 4 1 2 7 8 4 3 5 0 1 7 3 0 4 1 2 3 9 5 3 0 0 8 9 1 4 8 2 9 7 6 4 6 5\n",
      " 8 2 9 4 6 4 0 3 3 6 0 7 3 3 5 6 1 2 1 1 4 5 7 1 4 1 3 9 3 7 0 3 2 6 2 8 9\n",
      " 5 0 4 0 9 4 7 9 6 8 8 7 8 0 4 7 0 8 3 6 0 5 4 9 0 4 4 5 3 6 7 7 5 7 9 1 0\n",
      " 8 7 0 6 9 6 8 1 4 4 7 0 3 7 8 7 7 0 8 8 3 5 4 9 3 5 0 3 2 3 2 1 4 6 0 6 7\n",
      " 2 1 0 3 8 2 9 0 4 4 0 4 0 6 5 9 3 4 2 6 0 8 5 0 2 1 3 9 0 1 6 9 8 2 5 6 6\n",
      " 6 4 9 9 8 4 3 8 7 4 4 4 9 4 9 0 2 6 1 0 2 2 2 0 8 2 2 9 9 1 8 1 3 7 9 9 7\n",
      " 0 5 1 0 7 7 0 0 3 0 2 7 0 7 7 6 7 7 1 6 3 3 3 4 5 6 4 1 9 1 8 5 2 7 1 0 2\n",
      " 3 9 4 1 7 5 3 8 7 6 1 7 2 9 7 0 2 2 1 6 7 4 3 4 8 9 6 8 0 4 6 7 7 0 6 0 1\n",
      " 1] [2 5 9 0 3 7 0 1 1 3 5 6 6 6 2 4 7 6 7 5 1 9 4 1 0 0 6 2 4 2 0 3 1 7 4 5 6\n",
      " 6 1 2 5 5 0 5 2 6 5 2 0 1 1 1 8 3 7 0 4 1 6 4 5 7 1 3 1 7 3 2 7 3 3 4 1 7\n",
      " 8 1 7 9 8 8 9 8 8 5 3 8 0 9 3 3 8 0 1 1 8 4 0 6 1 0 3 2 3 0 7 9 1 5 9 7 8\n",
      " 3 1 9 4 3 2 9 2 8 6 2 7 4 4 7 9 3 8 5 3 5 0 8 4 3 9 0 9 5 3 8 4 6 5 8 5 7\n",
      " 5 1 7 7 7 8 2 9 8 0 7 0 6 7 1 5 7 5 6 0 1 8 1 1 3 6 6 7 6 3 1 6 0 6 9 2 9\n",
      " 9 5 1 3 4 7 6 8 5 8 1 9 2 6 4 9 8 3 8 7 9 1 7 9 0 6 8 7 8 7 1 6 6 0 6 9 1\n",
      " 4 9 8 2 0 9 4 7 7 3 2 4 9 5 0 7 1 5 0 3 7 2 3 7 6 4 1 8 2 1 6 2 5 5 4 6 1\n",
      " 6 7 8 1 7 2 0 8 7 3 2 9 0 1 8 4 1 4 0 6 4 0 2 9 4 7 4 6 7 5 0 6 6 0 2 1 3\n",
      " 6 9 2 8 7 0 5 8 5 3 1 1 0 8 9 0 0 6 6 4 2 4 4 4 2 6 6 2 8 5 2 6 1 9 9 0 7\n",
      " 5 7 5 2 1 8 0 3 9 4 3 8 3 9 7 2 8 8 5 8 1 9 2 9 9 0 0 4 6 3 5 4 8 2 6 2 7\n",
      " 3 2 4 0 8 9 2 6 2 1 1 0 9 9 6 3 9 7 9 3 9 2 1 0 4 9 3 5 9 9 1 4 4 7 4 8 0\n",
      " 6 1 0 4 1 6 7 4 1 9 9 8 4 4 4 9 8 5 8 4 3 7 5 0 6 5 3 8 9 1 0 8 7 3 8 0 5\n",
      " 4 8 7 5 4 5 0 1 1 2 1 5 9 9 2 1 2 7 1 4 0 6 0 5 5 7 3 8 6 8 9 0 1 6 0 8 1\n",
      " 8 7 3 2 2 1 4 7 0 9 2 7 1 4 1 2 5 5 8 1 7 0 6 1 3 1 9 2 4 5 3 9 3 6 9 3 6\n",
      " 6 4 2 7 0 6 4 5 1 9 5 9 5 5 8 9 9 2 9 1 7 0 9 3 9 6 7 7 2 4 4 5 5 3 8 0 7\n",
      " 4 2 1 2 5 7 1 9 2 0 6 7 1 8 0 5 4 0 1 9 7 0 7 2 5 6 7 9 0 5 3 9 8 1 0 3 6\n",
      " 7 4 6 6 7 4 6 7 7 2 1 6 6 7 1 9 7 3 1 2 4 4 9 7 3 4 3 4 0 6 0 0 3 0 1 0 7\n",
      " 9 3 1 5 2 5 3 4 6 2 1 0 7 5 5 8 3 7 5 7 1 2 5 4 1 4 0 3 8 3 1 8 9 9 7 6 8\n",
      " 6 8 2 7 7 6 5 3 9 7 7 2 6 9 0 1 0 2 3 8 3 5 4 0 4 2 6 3 0 7 1 1 6 4 9 1 0\n",
      " 2 4 0 4 1 2 7 8 4 3 5 0 1 7 5 0 4 1 2 3 9 5 3 0 0 8 9 1 4 8 7 9 7 6 4 6 5\n",
      " 7 2 4 4 6 4 0 3 3 6 0 7 3 3 5 6 1 2 1 1 4 5 7 1 4 1 5 9 3 7 0 3 2 6 2 8 9\n",
      " 5 0 9 0 9 4 7 9 6 8 8 7 2 0 4 7 0 8 3 6 0 5 4 9 0 4 4 5 3 6 7 7 5 7 9 1 0\n",
      " 8 7 0 6 9 6 5 1 4 4 3 0 3 7 8 7 7 2 8 2 3 5 4 9 9 5 0 3 2 3 2 1 4 6 0 6 7\n",
      " 2 2 0 3 8 2 9 0 2 4 0 4 0 6 3 9 3 4 2 2 0 8 9 6 2 2 3 9 0 1 6 7 8 2 5 6 6\n",
      " 6 4 9 9 8 4 3 8 7 4 4 4 9 6 7 2 2 6 1 0 2 2 2 0 8 2 8 4 7 1 8 1 3 7 8 9 7\n",
      " 0 5 1 0 7 7 0 0 3 0 8 7 0 7 7 6 9 7 1 5 3 5 3 4 5 6 4 1 9 1 5 5 2 3 1 0 6\n",
      " 3 9 4 1 7 5 3 8 7 6 1 7 2 9 7 0 6 2 1 6 7 4 3 4 8 9 6 2 0 4 6 7 7 0 6 0 9\n",
      " 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.892"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = make_predictions(X_test, W1, b1, W2, b2)\n",
    "get_accuracy(test_predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf4b18",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce4dc1b",
   "metadata": {},
   "source": [
    "These are some cases where the NN made incorrect predictions\n",
    "\n",
    "As I learn more I will come back and improve this NN\n",
    "\n",
    "<img src=\"Screenshot 2022-10-02 at 10.58.59.png\" width=\"300\" height=\"300\"> \n",
    "<img src=\"Screenshot 2022-10-02 at 10.59.11.png\" width=\"300\" height=\"300\"> \n",
    "<img src=\"Screenshot 2022-10-02 at 10.59.50.png\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e60057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
